{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from matchms.importing import load_from_json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "ROOT = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, ROOT)\n",
    "path_data = 'C:\\\\OneDrive - Netherlands eScience Center\\\\Project_Wageningen_iOMEGA\\\\matchms\\\\data\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of spectra: 112956\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(path_data,'gnps_positive_ionmode_cleaned_by_matchms_and_lookups.json')\n",
    "spectrums = load_from_json(filename)\n",
    "\n",
    "print(\"number of spectra:\", len(spectrums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(spectrums, \n",
    "            open(os.path.join(path_data,'gnps_positive_ionmode_cleaned_by_matchms_and_lookups.pickle'), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = os.path.join(path_data,'gnps_positive_ionmode_cleaned_by_matchms_and_lookups.pickle')\n",
    "with open(outfile, 'rb') as file:\n",
    "    spectrums = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.filtering import normalize_intensities\n",
    "from matchms.filtering import require_minimum_number_of_peaks\n",
    "from matchms.filtering import select_by_mz\n",
    "from matchms.filtering import select_by_relative_intensity\n",
    "from matchms.filtering import reduce_to_number_of_peaks\n",
    "from matchms.filtering import add_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining spectra: 95319\n"
     ]
    }
   ],
   "source": [
    "def post_process(s):\n",
    "    s = normalize_intensities(s)\n",
    "    s = select_by_mz(s, mz_from=10.0, mz_to=1000)\n",
    "    s = require_minimum_number_of_peaks(s, n_required=10)\n",
    "    return s\n",
    "\n",
    "# apply post processing steps to the data\n",
    "spectrums = [post_process(s) for s in spectrums]\n",
    "\n",
    "# omit spectrums that didn't qualify for analysis\n",
    "spectrums = [s for s in spectrums if s is not None]\n",
    "\n",
    "print(\"Number of remaining spectra:\", len(spectrums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(spectrums, \n",
    "            open(os.path.join(path_data,'gnps_positive_ionmode_processed.pickle'), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = os.path.join(path_data,'gnps_positive_ionmode_processed.pickle')\n",
    "with open(outfile, 'rb') as file:\n",
    "    spectrums = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of peaks in one spectrum: 228989\n",
      "Number of spectra with > 1000 peaks: 5481\n",
      "Number of spectra with > 2000 peaks: 2268\n",
      "Number of spectra with > 5000 peaks: 769\n",
      "Careful: Number of spectra with < 10 peaks: 0\n"
     ]
    }
   ],
   "source": [
    "number_of_peaks = [len(spec.peaks) for spec in spectrums]\n",
    "\n",
    "print(\"Maximum number of peaks in one spectrum:\", np.max(number_of_peaks))\n",
    "print(\"Number of spectra with > 1000 peaks:\", np.sum(np.array(number_of_peaks)>1000))\n",
    "print(\"Number of spectra with > 2000 peaks:\", np.sum(np.array(number_of_peaks)>2000))\n",
    "print(\"Number of spectra with > 5000 peaks:\", np.sum(np.array(number_of_peaks)>5000))\n",
    "print(\"Careful: Number of spectra with < 10 peaks:\", np.sum(np.array(number_of_peaks)<10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InChI=1S/C28H26N4O3/c1-28-26(34-3)17(29-2)12-20(35-28)31-18-10-6-4-8-14(18)22-23-16(13-30-27(23)33)21-15-9-5-7-11-19(15)32(28)25(21)24(22)31/h4-11,17,20,26,29H,12-13H2,1-3H3,(H,30,33)\n",
      "\n",
      "CNC1CC2OC(C)(C1OC)N1C3=CC=CC=C3C3=C4CNC(=O)C4=C4C5=C(C=CC=C5)N2C4=C13\n"
     ]
    }
   ],
   "source": [
    "ID = 102\n",
    "if spectrums[ID].get(\"inchi\") + spectrums[ID].get(\"smiles\"):\n",
    "    print(spectrums[ID].get(\"inchi\") + \"\\n\\n\" + spectrums[ID].get(\"smiles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_list = []\n",
    "annotation_list2 = []\n",
    "for i, s in enumerate(spectrums):\n",
    "    if s.get(\"inchi\") or s.get(\"smiles\"):\n",
    "        annotation_list.append((i, s.get(\"inchi\"), s.get(\"smiles\")))\n",
    "        \n",
    "    if s.get(\"inchikey\"):\n",
    "        annotation_list2.append((i, s.get(\"inchikey\")))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77144, 77091)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_list), len(annotation_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectrums_annotated = [s for s in spectrums if s.get(\"inchi\") or s.get(\"smiles\")]\n",
    "spectrums_annotated = [s for s in spectrums if s.get(\"inchikey\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77091, 95319)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spectrums_annotated), len(spectrums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to make spectrum vector for ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_size_fixed(number, d_bins):\n",
    "    return d_bins * number\n",
    "\n",
    "def bin_number_fixed(mz, d_bins):\n",
    "    \"\"\"Return binned position\"\"\"\n",
    "    return int(mz/d_bins)\n",
    "\n",
    "def bin_number_array_fixed(mz_array, d_bins, mz_min):\n",
    "    \"\"\"Return binned position\"\"\"\n",
    "    assert np.all(mz_array >= mz_min), \"Found peaks > mz_min.\"\n",
    "    bins = mz_array/d_bins\n",
    "    return (bins - bin_number_fixed(mz_min, d_bins)).astype(int)\n",
    "\n",
    "def set_d_bins_fixed(number_of_bins, mz_min=10.0, mz_max=1000.0):\n",
    "    return (mz_max - mz_min) / number_of_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_fixed(spectrums, d_bins, mz_min):\n",
    "    unique_peaks = set()\n",
    "    for spectrum in spectrums:\n",
    "        for mz in bin_number_array_fixed(spectrum.peaks.mz, d_bins, mz_min):\n",
    "            unique_peaks.add(mz)\n",
    "    unique_peaks = sorted(unique_peaks)\n",
    "    class_values = {}\n",
    "\n",
    "    for i, item in enumerate(unique_peaks):\n",
    "        class_values[item] = i\n",
    "    \n",
    "    return class_values, unique_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectors_fixed(spectrums, d_bins, mz_min=10.0, weight_power = 0.2):\n",
    "    \"\"\"Create documents.\"\"\"\n",
    "    documents = []\n",
    "    #documents_weights = []\n",
    "    #documents_bow = []\n",
    "\n",
    "    for spectrum in spectrums:\n",
    "        doc = bin_number_array_fixed(spectrum.peaks.mz, d_bins, mz_min=mz_min)\n",
    "        weights = spectrum.peaks.intensities ** weight_power\n",
    "        doc_bow = [class_values[x] for x in doc]\n",
    "        documents.append(list(zip(doc_bow, weights)))\n",
    "        #documents_weights.append(weights) \n",
    "        #documents_bow.append([class_values[x] for x in doc])\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def create_vector(document, vector_dim):\n",
    "    \"\"\"Create vector from document.\"\"\"\n",
    "    bow, weights = zip(*document)\n",
    "    vector_base = np.zeros((1, vector_dim))\n",
    "    vector = (np.array(weights) * to_categorical(np.array(bow)).T).max(axis=1)\n",
    "    vector_base[0, :vector.shape[0]] += vector\n",
    "    return vector_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.njit\n",
    "def create_full_vector(document, vector_dim):\n",
    "    \"\"\"Create full (sparse) vector from document.\"\"\"\n",
    "    full_vector = np.zeros((vector_dim))\n",
    "    for (ID, weight) in document:\n",
    "        full_vector[ID] = max(weight, full_vector[ID])\n",
    "    return full_vector\n",
    "\n",
    "def create_peak_dict(document):\n",
    "    peaks = {}\n",
    "    for (ID, weight) in document:\n",
    "        if ID in peaks:\n",
    "            peaks[ID] = max(weight, peaks[ID])\n",
    "        else:\n",
    "            peaks[ID] = weight\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reference scores (Tanimoto)\n",
    "- Check better alternatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.filtering.add_fingerprint import add_fingerprint\n",
    "\n",
    "spectrums_annotated = [add_fingerprint(s, fingerprint_type=\"daylight\", nbits=2048) for s in spectrums_annotated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(spectrums_annotated, \n",
    "            open(os.path.join(path_data,'gnps_positive_ionmode_annotated_w_fps_processed.pickle'), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = os.path.join(path_data,'gnps_positive_ionmode_annotated_w_fps_processed.pickle')\n",
    "with open(outfile, 'rb') as file:\n",
    "    spectrums_annotated = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inchikeys_list = []\n",
    "for s in spectrums_annotated:\n",
    "    inchikeys_list.append(s.get(\"inchikey\"))\n",
    "\n",
    "inchikeys_array = np.array(inchikeys_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inchi_list = []\n",
    "for s in spectrums_annotated:\n",
    "    inchi_list.append(s.get(\"inchi\"))\n",
    "\n",
    "inchi_array = np.array(inchi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14459"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inchikeys_unique = list(set(inchikeys_list))\n",
    "len(inchikeys_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(1)[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O=C(N1CCC=CC1=O)/C=C/C2=CC(OC)=C(OC)C(OC)=C2\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n",
      "\n",
      "most frequent: COc1cc(/C=C/C(=O)N2CCC=CC2=O)cc(OC)c1OC\n"
     ]
    }
   ],
   "source": [
    "inchikey = inchikeys_unique[1000]\n",
    "\n",
    "idx = np.where(inchikeys_array == inchikey)[0]\n",
    "for i in idx:\n",
    "    print(spectrums_annotated[i].get(\"smiles\") + \"\\n\")\n",
    "\n",
    "print(\"most frequent:\", most_frequent([spectrums_annotated[i].get(\"smiles\") for i in idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "inchi_mapping = []\n",
    "ID_mapping = []\n",
    "\n",
    "for inchikey in inchikeys_unique:\n",
    "    idx = np.where(inchikeys_array == inchikey)[0]\n",
    "    \n",
    "    inchi = most_frequent([spectrums_annotated[i].get(\"inchi\") for i in idx])\n",
    "    inchi_mapping.append(inchi)\n",
    "    ID = idx[np.where(inchi_array[idx] == inchi)[0][0]]\n",
    "    ID_mapping.append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>inchikey</th>\n",
       "      <th>inchi</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MYHSVHWQEVDFQT-RLIDIOMENA-N</td>\n",
       "      <td>InChI=1/C11H19NO10S2/c1-2-5(14)3-7(12-22-24(18...</td>\n",
       "      <td>75971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BKAWJIRCKVUVED-UHFFFAOYSA-N</td>\n",
       "      <td>InChI=1S/C6H9NOS/c1-5-6(2-3-8)9-4-7-5/h4,8H,2-...</td>\n",
       "      <td>17330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CXVGEDCSTKKODG-UHFFFAOYSA-N</td>\n",
       "      <td>InChI=1S/C14H12O6S/c1-20-12-8-11(15)10(7-13(12...</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>JAMSDVDUWQNQFZ-QNQJCTKXSA-N</td>\n",
       "      <td>InChI=1S/C52H102NO8P/c1-6-8-10-12-14-16-18-20-...</td>\n",
       "      <td>38937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ODHCTXKNWHHXJC-GSVOUGTGSA-N</td>\n",
       "      <td>InChI=1S/C5H7NO3/c7-4-2-1-3(6-4)5(8)9/h3H,1-2H...</td>\n",
       "      <td>46378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     inchikey  \\\n",
       "0           0  MYHSVHWQEVDFQT-RLIDIOMENA-N   \n",
       "1           1  BKAWJIRCKVUVED-UHFFFAOYSA-N   \n",
       "2           2  CXVGEDCSTKKODG-UHFFFAOYSA-N   \n",
       "3           3  JAMSDVDUWQNQFZ-QNQJCTKXSA-N   \n",
       "4           4  ODHCTXKNWHHXJC-GSVOUGTGSA-N   \n",
       "\n",
       "                                               inchi     ID  \n",
       "0  InChI=1/C11H19NO10S2/c1-2-5(14)3-7(12-22-24(18...  75971  \n",
       "1  InChI=1S/C6H9NOS/c1-5-6(2-3-8)9-4-7-5/h4,8H,2-...  17330  \n",
       "2  InChI=1S/C14H12O6S/c1-20-12-8-11(15)10(7-13(12...    422  \n",
       "3  InChI=1S/C52H102NO8P/c1-6-8-10-12-14-16-18-20-...  38937  \n",
       "4  InChI=1S/C5H7NO3/c7-4-2-1-3(6-4)5(8)9/h3H,1-2H...  46378  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"metadata_AllInchikeys_safe.csv\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14459,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[\"inchikey\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inchikey</th>\n",
       "      <th>inchi</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MYHSVHWQEVDFQT-RLIDIOMENA-N</td>\n",
       "      <td>InChI=1/C11H19NO10S2/c1-2-5(14)3-7(12-22-24(18...</td>\n",
       "      <td>75971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BKAWJIRCKVUVED-UHFFFAOYSA-N</td>\n",
       "      <td>InChI=1S/C6H9NOS/c1-5-6(2-3-8)9-4-7-5/h4,8H,2-...</td>\n",
       "      <td>17330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXVGEDCSTKKODG-UHFFFAOYSA-N</td>\n",
       "      <td>InChI=1S/C14H12O6S/c1-20-12-8-11(15)10(7-13(12...</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAMSDVDUWQNQFZ-QNQJCTKXSA-N</td>\n",
       "      <td>InChI=1S/C52H102NO8P/c1-6-8-10-12-14-16-18-20-...</td>\n",
       "      <td>38937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ODHCTXKNWHHXJC-GSVOUGTGSA-N</td>\n",
       "      <td>InChI=1S/C5H7NO3/c7-4-2-1-3(6-4)5(8)9/h3H,1-2H...</td>\n",
       "      <td>46378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      inchikey  \\\n",
       "0  MYHSVHWQEVDFQT-RLIDIOMENA-N   \n",
       "1  BKAWJIRCKVUVED-UHFFFAOYSA-N   \n",
       "2  CXVGEDCSTKKODG-UHFFFAOYSA-N   \n",
       "3  JAMSDVDUWQNQFZ-QNQJCTKXSA-N   \n",
       "4  ODHCTXKNWHHXJC-GSVOUGTGSA-N   \n",
       "\n",
       "                                               inchi     ID  \n",
       "0  InChI=1/C11H19NO10S2/c1-2-5(14)3-7(12-22-24(18...  75971  \n",
       "1  InChI=1S/C6H9NOS/c1-5-6(2-3-8)9-4-7-5/h4,8H,2-...  17330  \n",
       "2  InChI=1S/C14H12O6S/c1-20-12-8-11(15)10(7-13(12...    422  \n",
       "3  InChI=1S/C52H102NO8P/c1-6-8-10-12-14-16-18-20-...  38937  \n",
       "4  InChI=1S/C5H7NO3/c7-4-2-1-3(6-4)5(8)9/h3H,1-2H...  46378  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.DataFrame(list(zip(inchikeys_unique, inchi_mapping, ID_mapping)), columns=[\"inchikey\", \"inchi\", \"ID\"])\n",
    "metadata.to_csv(\"metadata_AllInchikeys.csv\")\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('InChI=1/C11H19NO10S2/c1-2-5(14)3-7(12-22-24(18,19)20)23-11-10(17)9(16)8(15)6(4-13)21-11/h2,5-6,8-11,13-17H,1,3-4H2,(H,18,19,20)/b12-7+/t5-,6+,8+,9-,10+,11u/m0/s1/f/h18H',\n",
       " 'MYHSVHWQEVDFQT-RLIDIOMENA-N',\n",
       " 14459)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "spectrums_annotated[75971].get(\"inchi\"), inchikeys_unique[0], len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14459,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.ID.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matchms.similarity import FingerprintSimilarity\n",
    "\n",
    "spectrums_represent = [spectrums_annotated[i] for i in metadata.ID.values]\n",
    "\n",
    "similarity_measure = FingerprintSimilarity(similarity_measure=\"jaccard\")\n",
    "scores_mol_similarity = similarity_measure.matrix(spectrums_represent, spectrums_represent)\n",
    "\n",
    "filename = os.path.join(path_data, \"similarities_AllInchikeys_daylight2048_jaccard.npy\")\n",
    "np.save(filename, scores_mol_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load reference scores (Tanimoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(path_data,'similarities_AllInchikeys_daylight2048_jaccard.npy')\n",
    "scores_mol_similarity = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14459, 14459)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_mol_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.09930716, 0.23619371, 0.20579111, 0.09101251],\n",
       "       [0.09930716, 1.        , 0.09296782, 0.08462867, 0.06852792],\n",
       "       [0.23619371, 0.09296782, 1.        , 0.14127144, 0.1026253 ],\n",
       "       [0.20579111, 0.08462867, 0.14127144, 1.        , 0.13392857],\n",
       "       [0.09101251, 0.06852792, 0.1026253 , 0.13392857, 1.        ],\n",
       "       [0.17615176, 0.07909605, 0.19981061, 0.11771429, 0.08438819],\n",
       "       [0.29403307, 0.10798946, 0.26704953, 0.20884521, 0.1674333 ],\n",
       "       [0.21709402, 0.10336239, 0.20993031, 0.16033755, 0.18435013],\n",
       "       [0.18895966, 0.07380074, 0.13493724, 0.18452381, 0.09683426],\n",
       "       [0.11052072, 0.08798283, 0.18963486, 0.08912387, 0.12227074]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_mol_similarity[:10,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3339,  5594, 10540], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID = 100\n",
    "np.where((scores_mol_similarity[ID,:] > 0.7) & (scores_mol_similarity[ID,:] < 0.99))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('InChI=1S/C20H39NO4S/c1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-20(22)21-18-19-26(23,24)25/h9-10H,2-8,11-19H2,1H3,(H,21,22)(H,23,24,25)/b10-9-',\n",
       " 'InChI=1S/C21H41NO4S/c1-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-21(23)22(2)19-20-27(24,25)26/h10-11H,3-9,12-20H2,1-2H3,(H,24,25,26)/b11-10-')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.loc[ID][\"inchi\"], metadata.loc[3339][\"inchi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data pipeline for training on AllInchikeys dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset into train/val/test fractions: 12290 1084 1085\n"
     ]
    }
   ],
   "source": [
    "split_ratio = (0.85, 0.075, 0.075)\n",
    "\n",
    "np.random.seed(111) #42\n",
    "\n",
    "N_label = len(inchikeys_unique)\n",
    "idx = np.arange(0, N_label)\n",
    "N_train = int(split_ratio[0] * N_label)\n",
    "N_val = int(split_ratio[1] * N_label)\n",
    "N_test = N_label - N_train - N_val\n",
    "print(\"Split dataset into train/val/test fractions:\", N_train, N_val, N_test)\n",
    "\n",
    "# Select training, validation, and test IDs:\n",
    "trainIDs = np.random.choice(idx, N_train, replace=False)\n",
    "valIDs = np.random.choice(list(set(idx) - set(trainIDs)), N_val, replace=False)\n",
    "testIDs = list(set(idx) - set(trainIDs) - set(valIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11925,  2413, 13459, 12783, 13292,  1785, 13828,   637,  7179,\n",
       "       10400])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainIDs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert spectra into binned spectra and peak dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_bins: 0.099\n",
      "Vector dimension: 9998\n"
     ]
    }
   ],
   "source": [
    "# Set binning --> LINEAR\n",
    "number_of_bins = 10000\n",
    "mz_max = 1000.0\n",
    "mz_min = 10.0\n",
    "weight_power = 0.5\n",
    "\n",
    "d_bins = set_d_bins_fixed(number_of_bins, mz_min=mz_min, mz_max=mz_max)\n",
    "print(\"d_bins:\", d_bins)\n",
    "\n",
    "class_values, unique_peaks = create_dictionary_fixed(spectrums_annotated, d_bins, mz_min)\n",
    "vector_dim = len(unique_peaks)\n",
    "print(\"Vector dimension:\", vector_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrums_binned = create_vectors_fixed(spectrums_annotated, d_bins, mz_min=mz_min, weight_power=weight_power)\n",
    "spectrums_binned_dicts = [create_peak_dict(spec) for spec in spectrums_binned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77091"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spectrums_binned_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try more balanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGeneratorAllInchikey(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, num_turns=1, dim=(10000,1), n_channels=1,\n",
    "                 n_classes=1, shuffle=True, score_array=None, inchikey_list= None, inchikey_mapping=None, \n",
    "                 same_prob_bins=[(0, 0.5), (0.5, 1)],\n",
    "                augment_peak_removal={\"max_removal\": 0.2, \"max_intensity\": 0.2},\n",
    "                 augment_intensity=0.1):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_turns = num_turns #number of go's through all IDs\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        assert score_array is not None, \"needs score array\"\n",
    "        self.score_array = score_array\n",
    "        assert inchikey_mapping is not None, \"needs inchikey mapping\"\n",
    "        self.inchikey_mapping = inchikey_mapping\n",
    "        self.inchikey_list = inchikey_list\n",
    "        self.score_array[np.isnan(score_array)] = 0\n",
    "        self.same_prob_bins = same_prob_bins\n",
    "        self.augment_peak_removal = augment_peak_removal\n",
    "        self.augment_intensity = augment_intensity\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(self.num_turns) * int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        # Go through all indexes\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Select subset of IDs\n",
    "        list_IDs_temp = []\n",
    "        for index in indexes:\n",
    "            ID1 = self.list_IDs[index]\n",
    "            prob_bins = self.same_prob_bins[np.random.choice(np.arange(len(self.same_prob_bins)))]\n",
    "            \n",
    "            idx = np.where((self.score_array[ID1, self.list_IDs] > prob_bins[0]) \n",
    "                           & (self.score_array[ID1, self.list_IDs] <= prob_bins[1]))[0]\n",
    "            if len(idx) > 0:\n",
    "                ID2 = self.list_IDs[np.random.choice(idx)]\n",
    "            else:\n",
    "                ID2 = self.list_IDs[np.random.randint(0, len(self.list_IDs))]\n",
    "                    \n",
    "            list_IDs_temp.append((ID1, ID2))\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        self.indexes = np.tile(np.arange(len(self.list_IDs)), int(self.num_turns))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_augmentation(self, document_dict):\n",
    "        \"\"\"Data augmentation.\"\"\"\n",
    "        idx = np.array([x for x in document_dict.keys()])\n",
    "        values = np.array([x for x in document_dict.values()])\n",
    "        if self.augment_peak_removal:\n",
    "            indices_select = np.where(values < self.augment_peak_removal[\"max_intensity\"])[0]\n",
    "            removal_part = np.random.random(1) * self.augment_peak_removal[\"max_removal\"]\n",
    "            indices_select = np.random.choice(indices_select,\n",
    "                                              int(np.ceil((1 - removal_part)*len(indices_select))))\n",
    "            indices = np.concatenate((indices_select,\n",
    "                                      np.where(values >= self.augment_peak_removal[\"max_intensity\"])[0]))\n",
    "            if len(indices) > 0:\n",
    "                idx = idx[indices]\n",
    "                values = values[indices]\n",
    "        if self.augment_intensity:\n",
    "            values = (1 - self.augment_intensity * 2 * (np.random.random(values.shape) - 0.5)) * values\n",
    "        return idx, values\n",
    "            \n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\" # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = [np.zeros((self.batch_size, 1, self.dim)) for i in range(2)]\n",
    "        y = np.zeros((self.batch_size,))\n",
    "        \n",
    "        # Generate data\n",
    "        for i, IDs in enumerate(list_IDs_temp):\n",
    "            # Select spectrum for respective inchikey\n",
    "            inchikey_1 = self.inchikey_mapping.loc[IDs[0]][\"inchikey\"]\n",
    "            inchikey_2 = self.inchikey_mapping.loc[IDs[1]][\"inchikey\"]\n",
    "            ID1 = np.random.choice(np.where(self.inchikey_list == inchikey_1)[0])\n",
    "            ID2 = np.random.choice(np.where(self.inchikey_list == inchikey_2)[0])\n",
    "            \n",
    "            ref_idx, ref_values = self.__data_augmentation(spectrums_binned_dicts[ID1])\n",
    "            X[0][i, 0, ref_idx] = ref_values\n",
    "            query_idx, query_values = self.__data_augmentation(spectrums_binned_dicts[ID2])\n",
    "            X[1][i, 0, query_idx] = query_values\n",
    "            y[i] = self.score_array[IDs[0], IDs[1]]\n",
    "            if np.isnan(y[i]):\n",
    "                y[i] = np.random.random(1)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_prob_bins = list(zip(0.1*np.arange(10), (0.1 + 0.1*np.arange(10))))\n",
    "\n",
    "train_generator = DataGeneratorAllInchikey(trainIDs, batch_size=16, num_turns=2, dim=(vector_dim), n_channels=1,\n",
    "                                           n_classes=1, shuffle=True,\n",
    "                                           score_array=scores_mol_similarity, \n",
    "                                           inchikey_mapping=metadata, inchikey_list = inchikeys_array,\n",
    "                                           same_prob_bins=same_prob_bins,\n",
    "                                           augment_peak_removal={\"max_removal\": 0.2, \"max_intensity\": 0.1**0.5},\n",
    "                                           augment_intensity=0.2)\n",
    "\n",
    "val_generator = DataGeneratorAllInchikey(valIDs, batch_size=16, num_turns=1, dim=(vector_dim), n_channels=1,\n",
    "                                         n_classes=1, shuffle=True,\n",
    "                                         score_array=scores_mol_similarity, \n",
    "                                         inchikey_mapping=metadata, inchikey_list = inchikeys_array,\n",
    "                                         same_prob_bins=same_prob_bins,\n",
    "                                         augment_peak_removal={\"max_removal\": 0.1, \"max_intensity\": 0.1**0.5},\n",
    "                                         augment_intensity=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 1, 9998),\n",
       " (16, 1, 9998),\n",
       " array([0.09530583, 0.18057554, 0.27840482, 0.40562249, 0.56427504,\n",
       "        0.19261745, 0.51131601, 0.1108871 , 0.16138125, 1.        ,\n",
       "        0.11970075, 0.31947332, 0.22798295, 0.88427673, 0.81027254,\n",
       "        0.20757825]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0].shape, A[1].shape, B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
